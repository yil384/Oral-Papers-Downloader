import requests
from bs4 import BeautifulSoup
import time
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from datetime import datetime
import argparse

class ICLRPaperDownloader:
    def __init__(self, year=2025, save_dir="iclr_papers"):
        self.base_url = "https://iclr.cc"
        self.year = year
        self.save_dir = f"{save_dir}_{year}"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(os.path.join(self.save_dir, "pdfs"), exist_ok=True)
        os.makedirs(os.path.join(self.save_dir, "metadata"), exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(self.save_dir, "download_log.txt")
        
    def log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    
    def get_paper_list(self, event_type="oral"):
        """è·å–ICLRè®ºæ–‡åˆ—è¡¨"""
        page_url = f"{self.base_url}/virtual/{self.year}/events/{event_type}"
        self.log(f"è·å–è®ºæ–‡åˆ—è¡¨: {page_url}")
        
        try:
            response = requests.get(page_url, headers=self.headers, timeout=30)
            if response.status_code != 200:
                self.log(f"è·å–é¡µé¢å¤±è´¥: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡å¡ç‰‡
            paper_divs = soup.find_all('div', class_='virtual-card')
            papers = []
            
            for idx, div in enumerate(paper_divs):
                try:
                    # è·å–æ ‡é¢˜å’Œé“¾æ¥
                    link = div.find('a', class_='small-title text-underline-hover')
                    if not link:
                        continue
                    
                    title = link.text.strip()
                    relative_url = link.get('href', '')
                    paper_url = self.base_url + relative_url if relative_url else ""
                    
                    # æå–è®ºæ–‡ID
                    paper_id = relative_url.split('/')[-1] if relative_url else str(idx)
                    
                    # è·å–è®ºæ–‡ç±»å‹
                    paper_type = event_type.upper()
                    type_div = div.find_next_sibling('div', class_='type_display_name_virtual_card')
                    if type_div:
                        paper_type = type_div.text.strip()
                    
                    # è·å–ä½œè€…ä¿¡æ¯
                    authors = ""
                    author_div = div.find_next_sibling('div', class_='author-str')
                    if author_div:
                        authors = author_div.text.strip()
                        # æ¸…ç†ä½œè€…åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                        authors = authors.replace(' Â· ', '; ').replace('&middot;', ';')
                    
                    # è·å–æ‘˜è¦
                    abstract = ""
                    details = div.find_next_sibling('details')
                    if not details:
                        # å°è¯•æŸ¥æ‰¾åç»­çš„detailsæ ‡ç­¾
                        for sibling in div.find_next_siblings():
                            if sibling.name == 'details':
                                details = sibling
                                break
                            elif sibling.name == 'div' and 'virtual-card' in sibling.get('class', []):
                                break
                    
                    if details:
                        abstract_div = details.find('div', class_='text-start p-4')
                        if abstract_div:
                            abstract = abstract_div.text.strip()
                    
                    papers.append({
                        'id': paper_id,
                        'title': title,
                        'paper_page_url': paper_url,
                        'authors': authors,
                        'abstract': abstract,
                        'type': paper_type,
                        'event_type': event_type,
                        'year': self.year
                    })
                    
                except Exception as e:
                    self.log(f"è§£æè®ºæ–‡ä¿¡æ¯å‡ºé”™: {e}")
                    continue
            
            self.log(f"æ‰¾åˆ° {len(papers)} ç¯‡ {event_type} è®ºæ–‡")
            return papers
            
        except Exception as e:
            self.log(f"è·å–è®ºæ–‡åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_openreview_url(self, paper_page_url, max_retries=3):
        """ä»ICLRè®ºæ–‡é¡µé¢è·å–OpenReviewé“¾æ¥"""
        for attempt in range(max_retries):
            try:
                response = requests.get(paper_page_url, headers=self.headers, timeout=30)
                if response.status_code != 200:
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    return None
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾OpenReviewé“¾æ¥ - å¤šç§æ–¹å¼
                # æ–¹å¼1: é€šè¿‡titleå±æ€§
                openreview_link = soup.find('a', {'title': 'OpenReview'})
                if openreview_link:
                    return openreview_link.get('href')
                
                # æ–¹å¼2: é€šè¿‡é“¾æ¥æ–‡æœ¬
                for link in soup.find_all('a'):
                    if 'OpenReview' in link.text:
                        href = link.get('href', '')
                        if 'openreview.net' in href:
                            return href
                
                # æ–¹å¼3: ç›´æ¥æŸ¥æ‰¾åŒ…å«openreview.netçš„é“¾æ¥
                for link in soup.find_all('a', href=True):
                    if 'openreview.net/forum' in link['href']:
                        return link['href']
                
                return None
                
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                self.log(f"è·å–OpenReviewé“¾æ¥å¤±è´¥: {e}")
                return None
        
        return None
    
    def get_pdf_url_from_openreview(self, openreview_url):
        """å°†OpenReview forumé“¾æ¥è½¬æ¢ä¸ºPDFé“¾æ¥"""
        if not openreview_url:
            return None
        
        # æå–IDå‚æ•°
        if 'forum?id=' in openreview_url:
            paper_id = openreview_url.split('forum?id=')[1].split('&')[0]
            pdf_url = f"https://openreview.net/pdf?id={paper_id}"
            return pdf_url
        
        return None
    
    def clean_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*', '\n', '\r', '\t']
        for char in illegal_chars:
            filename = filename.replace(char, '_')
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        filename = ' '.join(filename.split())
        
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename) > 150:
            filename = filename[:150]
        
        return filename.strip()
    
    def download_pdf(self, pdf_url, filename, max_retries=3):
        """ä¸‹è½½PDFæ–‡ä»¶"""
        if not pdf_url:
            return False
        
        for attempt in range(max_retries):
            try:
                response = requests.get(pdf_url, headers=self.headers, timeout=60, stream=True)
                if response.status_code == 200:
                    # æ£€æŸ¥å†…å®¹ç±»å‹
                    content_type = response.headers.get('content-type', '')
                    if 'pdf' not in content_type.lower() and 'octet-stream' not in content_type.lower():
                        self.log(f"è­¦å‘Š: å†…å®¹ç±»å‹ä¸æ˜¯PDF: {content_type}")
                    
                    # ä¸‹è½½æ–‡ä»¶
                    total_size = int(response.headers.get('content-length', 0))
                    with open(filename, 'wb') as f:
                        downloaded = 0
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                    
                    # éªŒè¯æ–‡ä»¶å¤§å°
                    if os.path.getsize(filename) < 1000:  # å°äº1KBå¯èƒ½æ˜¯é”™è¯¯
                        os.remove(filename)
                        return False
                    
                    return True
                elif response.status_code == 404:
                    return False
                else:
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                self.log(f"ä¸‹è½½å¤±è´¥: {e}")
        
        return False
    
    def download_single_paper(self, paper):
        """ä¸‹è½½å•ç¯‡è®ºæ–‡"""
        paper_id = paper['id']
        title = paper['title']
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        safe_title = self.clean_filename(title)
        pdf_filename = os.path.join(self.save_dir, "pdfs", f"ICLR{self.year}_{paper_id}_{safe_title}.pdf")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°åˆç†ï¼Œè·³è¿‡
        if os.path.exists(pdf_filename) and os.path.getsize(pdf_filename) > 10000:
            paper['local_pdf_path'] = pdf_filename
            paper['download_status'] = 'exists'
            return {'status': 'exists', 'paper': paper}
        
        # è·å–OpenReviewé“¾æ¥
        openreview_url = self.get_openreview_url(paper['paper_page_url'])
        if not openreview_url:
            paper['download_status'] = 'no_openreview'
            return {'status': 'no_openreview', 'paper': paper}
        
        # è·å–PDFé“¾æ¥
        pdf_url = self.get_pdf_url_from_openreview(openreview_url)
        if not pdf_url:
            paper['download_status'] = 'no_pdf_url'
            return {'status': 'no_pdf_url', 'paper': paper}
        
        # æ›´æ–°è®ºæ–‡ä¿¡æ¯
        paper['openreview_url'] = openreview_url
        paper['pdf_url'] = pdf_url
        
        # ä¸‹è½½PDF
        if self.download_pdf(pdf_url, pdf_filename):
            paper['local_pdf_path'] = pdf_filename
            paper['download_status'] = 'success'
            paper['file_size'] = os.path.getsize(pdf_filename)
            return {'status': 'success', 'paper': paper}
        else:
            paper['download_status'] = 'download_failed'
            return {'status': 'download_failed', 'paper': paper}
    
    def download_all_papers(self, papers, max_workers=3):
        """å¹¶è¡Œä¸‹è½½æ‰€æœ‰è®ºæ–‡"""
        self.log(f"å¼€å§‹ä¸‹è½½ {len(papers)} ç¯‡è®ºæ–‡çš„PDF...")
        
        results = {
            'success': [],
            'exists': [],
            'failed': []
        }
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_paper = {
                executor.submit(self.download_single_paper, paper): paper 
                for paper in papers
            }
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
            with tqdm(total=len(papers), desc="ä¸‹è½½è¿›åº¦") as pbar:
                for future in as_completed(future_to_paper):
                    try:
                        result = future.result()
                        status = result['status']
                        paper = result['paper']
                        
                        if status == 'success':
                            results['success'].append(paper)
                            tqdm.write(f"âœ… ä¸‹è½½æˆåŠŸ: {paper['title'][:60]}...")
                        elif status == 'exists':
                            results['exists'].append(paper)
                            tqdm.write(f"â­ï¸  å·²å­˜åœ¨: {paper['title'][:60]}...")
                        else:
                            results['failed'].append(paper)
                            tqdm.write(f"âŒ å¤±è´¥({status}): {paper['title'][:60]}...")
                        
                        pbar.update(1)
                        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                        
                    except Exception as e:
                        self.log(f"å¤„ç†ä¸‹è½½ç»“æœæ—¶å‡ºé”™: {e}")
                        pbar.update(1)
        
        return results
    
    def save_metadata(self, papers, filename):
        """ä¿å­˜è®ºæ–‡å…ƒæ•°æ®"""
        filepath = os.path.join(self.save_dir, "metadata", filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(papers, f, ensure_ascii=False, indent=2)
        self.log(f"å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
    
    def generate_report(self, results):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report_file = os.path.join(self.save_dir, "download_report.md")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# ICLR {self.year} Papers Download Report\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            f.write("## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- âœ… æˆåŠŸä¸‹è½½: {len(results['success'])} ç¯‡\n")
            f.write(f"- â­ï¸  å·²å­˜åœ¨: {len(results['exists'])} ç¯‡\n")
            f.write(f"- âŒ ä¸‹è½½å¤±è´¥: {len(results['failed'])} ç¯‡\n")
            f.write(f"- ğŸ“ æ€»è®¡: {len(results['success']) + len(results['exists']) + len(results['failed'])} ç¯‡\n\n")
            
            # æˆåŠŸä¸‹è½½çš„è®ºæ–‡åˆ—è¡¨
            if results['success']:
                f.write("## âœ… æˆåŠŸä¸‹è½½çš„è®ºæ–‡\n\n")
                for i, paper in enumerate(results['success'], 1):
                    f.write(f"{i}. **{paper['title']}**\n")
                    f.write(f"   - ä½œè€…: {paper.get('authors', 'N/A')}\n")
                    f.write(f"   - OpenReview: [{paper.get('openreview_url', 'N/A')}]({paper.get('openreview_url', '#')})\n")
                    f.write(f"   - æ–‡ä»¶å¤§å°: {paper.get('file_size', 0) / 1024 / 1024:.2f} MB\n\n")
            
            # å¤±è´¥çš„è®ºæ–‡åˆ—è¡¨
            if results['failed']:
                f.write("## âŒ ä¸‹è½½å¤±è´¥çš„è®ºæ–‡\n\n")
                for i, paper in enumerate(results['failed'], 1):
                    f.write(f"{i}. **{paper['title']}**\n")
                    f.write(f"   - å¤±è´¥åŸå› : {paper.get('download_status', 'unknown')}\n")
                    f.write(f"   - è®ºæ–‡é¡µé¢: [{paper['paper_page_url']}]({paper['paper_page_url']})\n\n")
        
        self.log(f"ä¸‹è½½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def run(self, event_types=None):
        """è¿è¡Œå®Œæ•´çš„ä¸‹è½½æµç¨‹"""
        if event_types is None:
            event_types = ["oral"]  # é»˜è®¤åªä¸‹è½½oralè®ºæ–‡
        
        self.log("="*60)
        self.log(f"ICLR {self.year} è®ºæ–‡ä¸‹è½½å™¨å¯åŠ¨")
        self.log("="*60)
        
        all_papers = []
        
        # è·å–æ‰€æœ‰ç±»å‹çš„è®ºæ–‡
        for event_type in event_types:
            self.log(f"\nè·å– {event_type} è®ºæ–‡åˆ—è¡¨...")
            papers = self.get_paper_list(event_type)
            all_papers.extend(papers)
            
            # ä¿å­˜æ¯ç§ç±»å‹çš„è®ºæ–‡åˆ—è¡¨
            if papers:
                self.save_metadata(papers, f"iclr_{self.year}_{event_type}_list.json")
        
        if not all_papers:
            self.log("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®ºæ–‡")
            return
        
        self.log(f"\næ€»å…±æ‰¾åˆ° {len(all_papers)} ç¯‡è®ºæ–‡")
        
        # ä¿å­˜å®Œæ•´çš„è®ºæ–‡åˆ—è¡¨
        self.save_metadata(all_papers, f"iclr_{self.year}_all_papers_list.json")
        
        # ä¸‹è½½æ‰€æœ‰è®ºæ–‡
        results = self.download_all_papers(all_papers, max_workers=3)
        
        # ä¿å­˜ä¸‹è½½ç»“æœ
        if results['success']:
            self.save_metadata(results['success'], f"iclr_{self.year}_downloaded.json")
        if results['failed']:
            self.save_metadata(results['failed'], f"iclr_{self.year}_failed.json")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(results)
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self.log("\n" + "="*60)
        self.log("ä¸‹è½½å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ï¼š")
        self.log(f"âœ… æˆåŠŸä¸‹è½½: {len(results['success'])} ç¯‡")
        self.log(f"â­ï¸  å·²å­˜åœ¨: {len(results['exists'])} ç¯‡")
        self.log(f"âŒ ä¸‹è½½å¤±è´¥: {len(results['failed'])} ç¯‡")
        self.log(f"ğŸ“ PDFä¿å­˜ä½ç½®: {os.path.join(self.save_dir, 'pdfs')}")
        self.log("="*60)

def main():
    parser = argparse.ArgumentParser(description="NeurIPS paper downloader")
    parser.add_argument('--year', type=int, default=2024, help='Year of the NeurIPS conference (default: 2024)')
    args = parser.parse_args()

    # åˆ›å»ºICLRä¸‹è½½å™¨
    downloader = ICLRPaperDownloader(
        year=args.year,
        save_dir=f"iclr_{args.year}_papers"
    )
    
    # ä¸‹è½½oralè®ºæ–‡
    downloader.run(event_types=["oral"])
    
    # å¦‚æœè¦ä¸‹è½½æ‰€æœ‰ç±»å‹çš„è®ºæ–‡ï¼Œå¯ä»¥ä½¿ç”¨ï¼š
    # downloader.run(event_types=["oral", "poster", "spotlight"])

if __name__ == "__main__":
    main()