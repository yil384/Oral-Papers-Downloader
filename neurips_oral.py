import requests
from bs4 import BeautifulSoup
import time
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse

class NeurIPSPaperDownloader:
    def __init__(self, year=2025, save_dir="neurips_papers"):
        self.base_url = "https://neurips.cc"
        self.year = year
        self.save_dir = save_dir
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, "pdfs"), exist_ok=True)
        
    def get_paper_list(self, event_type="oral"):
        """è·å–è®ºæ–‡åˆ—è¡¨"""
        page_url = f"{self.base_url}/virtual/{self.year}/events/{event_type}"
        print(f"æ­£åœ¨è·å–è®ºæ–‡åˆ—è¡¨: {page_url}")
        
        response = requests.get(page_url, headers=self.headers)
        if response.status_code != 200:
            print(f"è·å–é¡µé¢å¤±è´¥: {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        paper_divs = soup.find_all('div', class_='virtual-card')
        
        papers = []
        for div in paper_divs:
            try:
                link = div.find('a', class_='small-title text-underline-hover')
                if not link:
                    continue
                
                title = link.text.strip()
                relative_url = link.get('href', '')
                paper_url = self.base_url + relative_url if relative_url else ""
                
                # è·å–è®ºæ–‡IDï¼ˆä»URLä¸­æå–ï¼‰
                paper_id = relative_url.split('/')[-1] if relative_url else ""
                
                # è·å–ä½œè€…ä¿¡æ¯
                authors = ""
                author_div = div.find_next_sibling('div', class_='author-str')
                if author_div:
                    authors = author_div.text.strip().replace(' Â· ', '; ')
                
                papers.append({
                    'id': paper_id,
                    'title': title,
                    'paper_page_url': paper_url,
                    'authors': authors,
                    'event_type': event_type
                })
                
            except Exception as e:
                print(f"è§£æè®ºæ–‡ä¿¡æ¯å‡ºé”™: {e}")
                continue
        
        print(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def get_openreview_url(self, paper_page_url):
        """ä»è®ºæ–‡é¡µé¢è·å–OpenReviewé“¾æ¥"""
        try:
            response = requests.get(paper_page_url, headers=self.headers)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾OpenReviewé“¾æ¥
            openreview_link = soup.find('a', {'title': 'OpenReview'})
            if openreview_link:
                return openreview_link.get('href')
            
            # å¤‡é€‰æ–¹æ³•ï¼šæŸ¥æ‰¾åŒ…å«openreview.netçš„é“¾æ¥
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                if 'openreview.net/forum' in link['href']:
                    return link['href']
            
            return None
        except Exception as e:
            print(f"è·å–OpenReviewé“¾æ¥å¤±è´¥: {e}")
            return None
    
    def get_pdf_url_from_openreview(self, openreview_url):
        """å°†OpenReview forumé“¾æ¥è½¬æ¢ä¸ºPDFé“¾æ¥"""
        if not openreview_url:
            return None
        
        # æå–IDå‚æ•°
        if 'forum?id=' in openreview_url:
            paper_id = openreview_url.split('forum?id=')[1].split('&')[0]
            pdf_url = f"https://openreview.net/pdf?id={paper_id}"
            return pdf_url
        
        return None
    
    def download_pdf(self, pdf_url, filename, max_retries=3):
        """ä¸‹è½½PDFæ–‡ä»¶"""
        if not pdf_url:
            return False
        
        for attempt in range(max_retries):
            try:
                response = requests.get(pdf_url, headers=self.headers, timeout=30, stream=True)
                if response.status_code == 200:
                    with open(filename, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    return True
                elif response.status_code == 404:
                    print(f"PDFä¸å­˜åœ¨: {pdf_url}")
                    return False
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                print(f"ä¸‹è½½å¤±è´¥: {e}")
                return False
        
        return False
    
    def clean_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢Windowsæ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
        for char in illegal_chars:
            filename = filename.replace(char, '_')
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename) > 200:
            filename = filename[:200]
        return filename.strip()
    
    def download_single_paper(self, paper):
        """ä¸‹è½½å•ç¯‡è®ºæ–‡"""
        paper_id = paper['id']
        title = paper['title']
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        safe_title = self.clean_filename(title)
        pdf_filename = os.path.join(self.save_dir, "pdfs", f"{paper_id}_{safe_title}.pdf")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
        if os.path.exists(pdf_filename):
            return {'status': 'exists', 'paper': paper}
        
        # è·å–OpenReviewé“¾æ¥
        openreview_url = self.get_openreview_url(paper['paper_page_url'])
        if not openreview_url:
            return {'status': 'no_openreview', 'paper': paper}
        
        # è·å–PDFé“¾æ¥
        pdf_url = self.get_pdf_url_from_openreview(openreview_url)
        if not pdf_url:
            return {'status': 'no_pdf_url', 'paper': paper}
        
        # ä¸‹è½½PDF
        paper['openreview_url'] = openreview_url
        paper['pdf_url'] = pdf_url
        
        if self.download_pdf(pdf_url, pdf_filename):
            paper['local_pdf_path'] = pdf_filename
            return {'status': 'success', 'paper': paper}
        else:
            return {'status': 'download_failed', 'paper': paper}
    
    def download_all_papers(self, papers, max_workers=5):
        """å¹¶è¡Œä¸‹è½½æ‰€æœ‰è®ºæ–‡"""
        print(f"\nå¼€å§‹ä¸‹è½½ {len(papers)} ç¯‡è®ºæ–‡çš„PDF...")
        
        results = {
            'success': [],
            'exists': [],
            'failed': []
        }
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_paper = {
                executor.submit(self.download_single_paper, paper): paper 
                for paper in papers
            }
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
            with tqdm(total=len(papers), desc="ä¸‹è½½è¿›åº¦") as pbar:
                for future in as_completed(future_to_paper):
                    result = future.result()
                    status = result['status']
                    paper = result['paper']
                    
                    if status == 'success':
                        results['success'].append(paper)
                        tqdm.write(f"âœ… ä¸‹è½½æˆåŠŸ: {paper['title'][:50]}...")
                    elif status == 'exists':
                        results['exists'].append(paper)
                        tqdm.write(f"â­ï¸  å·²å­˜åœ¨: {paper['title'][:50]}...")
                    else:
                        results['failed'].append(paper)
                        tqdm.write(f"âŒ ä¸‹è½½å¤±è´¥ ({status}): {paper['title'][:50]}...")
                    
                    pbar.update(1)
                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return results
    
    def save_metadata(self, papers, filename="paper_metadata.json"):
        """ä¿å­˜è®ºæ–‡å…ƒæ•°æ®"""
        filepath = os.path.join(self.save_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(papers, f, ensure_ascii=False, indent=2)
        print(f"å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
    
    def run(self, event_types=None):
        """è¿è¡Œå®Œæ•´çš„ä¸‹è½½æµç¨‹"""
        if event_types is None:
            event_types = ["oral"]  # é»˜è®¤åªä¸‹è½½oralè®ºæ–‡
        
        all_papers = []
        
        # è·å–æ‰€æœ‰ç±»å‹çš„è®ºæ–‡
        for event_type in event_types:
            print(f"\nè·å– {event_type} è®ºæ–‡...")
            papers = self.get_paper_list(event_type)
            all_papers.extend(papers)
        
        if not all_papers:
            print("æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡")
            return
        
        print(f"\næ€»å…±æ‰¾åˆ° {len(all_papers)} ç¯‡è®ºæ–‡")
        
        # ä¸‹è½½æ‰€æœ‰è®ºæ–‡
        results = self.download_all_papers(all_papers, max_workers=3)
        
        # ä¿å­˜å…ƒæ•°æ®
        self.save_metadata(results['success'] + results['exists'], "downloaded_papers.json")
        if results['failed']:
            self.save_metadata(results['failed'], "failed_papers.json")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "="*50)
        print("ä¸‹è½½ç»Ÿè®¡:")
        print(f"âœ… æˆåŠŸä¸‹è½½: {len(results['success'])} ç¯‡")
        print(f"â­ï¸  å·²å­˜åœ¨: {len(results['exists'])} ç¯‡")
        print(f"âŒ ä¸‹è½½å¤±è´¥: {len(results['failed'])} ç¯‡")
        print(f"ğŸ“ PDFä¿å­˜ä½ç½®: {os.path.join(self.save_dir, 'pdfs')}")
        print("="*50)

def main():
    parser = argparse.ArgumentParser(description="NeurIPS paper downloader")
    parser.add_argument('--year', type=int, default=2024, help='Year of the NeurIPS conference (default: 2024)')
    args = parser.parse_args()

    # åˆ›å»ºä¸‹è½½å™¨å®ä¾‹
    downloader = NeurIPSPaperDownloader(
        year=args.year,
        save_dir=f"neurips_{args.year}_papers"
    )
    
    # è¿è¡Œä¸‹è½½
    # å¯ä»¥æŒ‡å®šè¦ä¸‹è½½çš„è®ºæ–‡ç±»å‹ï¼š["oral", "poster", "spotlight"]
    downloader.run(event_types=["oral"])
    
    # å¦‚æœè¦ä¸‹è½½æ‰€æœ‰ç±»å‹çš„è®ºæ–‡ï¼š
    # downloader.run(event_types=["oral", "poster", "spotlight"])

if __name__ == "__main__":
    main()